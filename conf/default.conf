DB {
    host = "app-postgres"
    port = "5432"
    baseName = "vacstorage"
    userName = "postgres"
    userPassword = "1234"
}

FS {
    url = "hdfs://namenode:9000"
    path = "VacStorage/"
}

Spark {
    name = "SparkApp"
    master = "local[*]"
}

Arguments {
    gj {
        headers {
            userAgent = "MyApp/1.0"
        }
        pageLimit = 10
        rawPartitions = 1
        transformPartitions = 1
    }

    gm {
        headers {
            userAgent = "MyApp/1.0"
        }
        vacsLimit = 50
        rawPartitions = 1
        transformPartitions = 1
    }

    hh {
        headers {
            userAgent = "MyApp/1.0"
        }
        fieldId = 11  # 11 -> it-vacancies
        vacsPerPage = 100  # <= 100
        pageLimit = 8  # <= 20
        urlsPerSecond = 15
        rawPartitions = 1
        transformPartitions = 1
    }
}

Dags {   
    ScalaVersion = "2.12"
    StandartDate = "{{ execution_date.strftime('%Y-%m-%d') }}"
    SparkConnId = "SPARK_CONN"
    PostgresConnId = "POSTGRES_CONN"
    TimeZone = "Asia/Krasnoyarsk"

    gj {
        fileName = ${Dags.StandartDate}
        schedule = ""       
    }

    gm {
        fileName = ${Dags.StandartDate}
        schedule = ""
    }

    hh {
        currency {
            schedule = ""
        }

        dictionaries {
            schedule = ""
        }

        fileName = ${Dags.StandartDate}
        schedule = "" 
    }

    refreshMatView {
        schedule = ""
    }

    RawDataStorageTime = 60
    TransDataStorageTime = 120

    deleteExpiredData {
        schedule = ""
        hdfsPrefix = ""

        hh {
            raw = ${Dags.RawDataStorageTime}
            trans = ${Dags.TransDataStorageTime}
        }

        gj {
            raw = ${Dags.RawDataStorageTime}
            trans = ${Dags.TransDataStorageTime}
        }

        gm {
            raw = ${Dags.RawDataStorageTime}
            trans = ${Dags.TransDataStorageTime}
        }
    }
}